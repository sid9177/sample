import os
import streamlit as st
from openai import AzureOpenAI
import PyPDF2
import pandas as pd
import numpy as np
import faiss
import io

# Optional: for chunking or token counting
# pip install tiktoken
# import tiktoken  

# ------------------------------
# 1. Azure & Global Settings
# ------------------------------
EMBEDDING_DEPLOYMENT_NAME = "my-embedding-model"  # e.g. "text-embedding-ada-002" or your Azure embedding
CHAT_DEPLOYMENT_NAME = "Cata-GPT4-o"              # Your Azure ChatGPT model deployment
API_VERSION = "2024-05-01-preview"                # Example version
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT", "https://YOUR_AZURE_OPENAI_ENDPOINT_HERE")
AZURE_OPENAI_KEY = os.environ.get("AZURE_OPENAI_API_KEY", "YOUR_AZURE_OPENAI_API_KEY_HERE")


# Create a single AzureOpenAI client
client = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=API_VERSION,
)

# ------------------------------
# 2. Helper Functions
# ------------------------------
def extract_text_from_pdf(file) -> str:
    pdf_reader = PyPDF2.PdfReader(file)
    all_text = []
    for page in pdf_reader.pages:
        text = page.extract_text()
        if text:
            all_text.append(text)
    return "\n".join(all_text)


def extract_text_from_excel(file) -> str:
    xls = pd.ExcelFile(file)
    excel_text = []
    for sheet_name in xls.sheet_names:
        df = xls.parse(sheet_name)
        excel_text.append(df.to_csv(index=False))
    return "\n".join(excel_text)


def extract_text_from_csv(file) -> str:
    df = pd.read_csv(file)
    return df.to_csv(index=False)


def chunk_text(text, chunk_size=1000, overlap=200):
    """
    Splits 'text' into chunks of size 'chunk_size' with 'overlap' tokens/chars.
    For simplicity, we'll treat them as characters. For robust approach, use a tokenizer library.
    """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += (chunk_size - overlap)
    return chunks


def embed_text_azure_openai(client, texts):
    """
    Embed a list of strings 'texts' using Azure OpenAI embeddings.
    Returns a list of embedding vectors (numpy arrays).
    """
    # Each text input -> an embedding
    # For Azure, set engine=EMBEDDING_DEPLOYMENT_NAME
    response = client.create_embedding(
        engine=EMBEDDING_DEPLOYMENT_NAME,
        input=texts
    )
    # We'll parse the returned data
    # response.data = list of { "index": i, "embedding": [...], "object": "embedding"}
    embeddings = []
    for obj in response.data:
        embeddings.append(np.array(obj["embedding"], dtype=np.float32))
    return embeddings


def build_faiss_index(embeddings):
    """
    Build a FAISS index from a list of embedding vectors.
    Returns (index, dim).
    """
    if not embeddings:
        return None, None
    dim = embeddings[0].shape[0]
    # We'll use a simple Flat index with inner product (cosine similarity)
    index = faiss.IndexFlatIP(dim)
    # Stack embeddings into a 2D array
    embs_array = np.stack(embeddings)
    index.add(embs_array)
    return index, dim


def chat_completion_with_context(client, user_query, relevant_chunks):
    """
    Construct a prompt/system message from the top relevant chunks,
    then call the ChatCompletion endpoint.
    """
    # Combine top chunks
    context_text = "\n\n".join(relevant_chunks)
    system_prompt = (
        "You are a helpful assistant. Use the following context to answer the user's question.\n\n"
        f"Context:\n{context_text}\n\n"
        "Only answer using the context above. If you're not sure, say you don't know.\n"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query},
    ]

    response = client.create_chat_completion(
        engine=CHAT_DEPLOYMENT_NAME,
        messages=messages,
        temperature=0.7,
    )
    return response.choices[0].message.content


# -----------------------------------------
# 3. Streamlit RAG Demo
# -----------------------------------------
def main():
    st.title("RAG Demo with Azure OpenAI + FAISS")

    # We'll store the chunks and their embeddings in session_state
    if "chunks" not in st.session_state:
        st.session_state.chunks = []
    if "chunk_embeddings" not in st.session_state:
        st.session_state.chunk_embeddings = []
    if "faiss_index" not in st.session_state:
        st.session_state.faiss_index = None
    if "doc_sources" not in st.session_state:
        st.session_state.doc_sources = []  # store (chunk_text, doc_name)

    # 1) Document Upload & Chunking
    st.header("Upload Documents and Build Index")

    policy_file = st.file_uploader("Upload Policy PDF", type=["pdf"])
    methodology_file = st.file_uploader("Upload Methodology PDF", type=["pdf"])
    formula_file = st.file_uploader("Upload Formula Excel", type=["xlsx", "xls"])
    csv_file = st.file_uploader("Upload CSV Data", type=["csv"])

    if st.button("Process Documents"):
        # Clear old index
        st.session_state.chunks = []
        st.session_state.chunk_embeddings = []
        st.session_state.faiss_index = None
        st.session_state.doc_sources = []

        # Policy
        if policy_file is not None:
            policy_text = extract_text_from_pdf(policy_file)
            for chunk in chunk_text(policy_text):
                st.session_state.chunks.append(chunk)
                st.session_state.doc_sources.append((chunk, "Policy Document"))

        # Methodology
        if methodology_file is not None:
            methodology_text = extract_text_from_pdf(methodology_file)
            for chunk in chunk_text(methodology_text):
                st.session_state.chunks.append(chunk)
                st.session_state.doc_sources.append((chunk, "Methodology Document"))

        # Formula
        if formula_file is not None:
            formula_text = extract_text_from_excel(formula_file)
            for chunk in chunk_text(formula_text):
                st.session_state.chunks.append(chunk)
                st.session_state.doc_sources.append((chunk, "Formula Excel"))

        # CSV
        if csv_file is not None:
            csv_text = extract_text_from_csv(csv_file)
            for chunk in chunk_text(csv_text):
                st.session_state.chunks.append(chunk)
                st.session_state.doc_sources.append((chunk, "CSV File"))

        st.write(f"Total chunks created: {len(st.session_state.chunks)}")

        # 2) Embed and Build FAISS
        if st.session_state.chunks:
            st.write("Generating embeddings & building FAISS index...")
            # We'll embed in batch, for demonstration we do it in one call if not too large
            # If you have lots of chunks, consider batching.
            embeddings = []
            BATCH_SIZE = 5
            for i in range(0, len(st.session_state.chunks), BATCH_SIZE):
                batch = st.session_state.chunks[i : i + BATCH_SIZE]
                embs = client.create_embedding(
                    engine=EMBEDDING_DEPLOYMENT_NAME,
                    input=batch
                )
                for obj in embs.data:
                    embeddings.append(np.array(obj["embedding"], dtype=np.float32))

            # Save embeddings
            st.session_state.chunk_embeddings = embeddings
            # Build index
            index, dim = build_faiss_index(embeddings)
            st.session_state.faiss_index = index
            st.success("FAISS index built successfully!")

    st.divider()

    # 3) Query Interface
    st.header("Ask Questions")
    user_query = st.text_input("Enter your query:")
    if st.button("Search & Answer"):
        # We'll embed the user query
        if not st.session_state.faiss_index or not st.session_state.chunk_embeddings:
            st.warning("No documents have been processed yet. Please upload & process documents first.")
            return

        query_emb = client.create_embedding(
            engine=EMBEDDING_DEPLOYMENT_NAME,
            input=[user_query]
        ).data[0].embedding
        query_emb = np.array(query_emb, dtype=np.float32).reshape(1, -1)

        # Similarity search in FAISS
        index = st.session_state.faiss_index
        top_k = 3
        D, I = index.search(query_emb, top_k)  # distances, indices

        # Retrieve the top chunks
        top_chunks = []
        for idx in I[0]:
            chunk_text = st.session_state.chunks[idx]
            top_chunks.append(chunk_text)

        # Chat Completion with relevant chunks
        assistant_reply = chat_completion_with_context(
            client,
            user_query,
            top_chunks
        )

        st.write("**Assistant**:", assistant_reply)

        # Optional: Show the sources
        st.subheader("Top Relevant Chunks:")
        for i, idx in enumerate(I[0]):
            st.markdown(f"**Rank {i+1}, Score {D[0][i]:.4f}**")
            st.text_area(
                label=f"Chunk from {st.session_state.doc_sources[idx][1]}",
                value=st.session_state.doc_sources[idx][0],
                height=100
            )

if __name__ == "__main__":
    main()
