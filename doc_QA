import os
import streamlit as st
from openai import AzureOpenAI
import PyPDF2
import pandas as pd
import numpy as np
import faiss
import io
import tiktoken  # for token-based chunking

# ------------------------------------------------------------------------------
# 1. Azure & Global Settings
# ------------------------------------------------------------------------------
EMBEDDING_MODEL_NAME = "text-embedding-ada-002"  # or your actual Azure embedding deployment name
CHAT_MODEL_NAME = "Cata-GPT4-o"                  # Your Azure ChatGPT model name/deployment
API_VERSION = "2024-05-01-preview"
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT", "https://YOUR_AZURE_OPENAI_ENDPOINT_HERE")
AZURE_OPENAI_KEY = os.environ.get("AZURE_OPENAI_API_KEY", "YOUR_AZURE_OPENAI_API_KEY_HERE")

# Initialize a single AzureOpenAI client
client = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=API_VERSION,
)


# ------------------------------------------------------------------------------
# 2. Helper Functions
# ------------------------------------------------------------------------------
def extract_text_from_pdf(file) -> str:
    pdf_reader = PyPDF2.PdfReader(file)
    all_text = []
    for page in pdf_reader.pages:
        text = page.extract_text()
        if text:
            all_text.append(text)
    return "\n".join(all_text)


def extract_text_from_excel(file) -> str:
    xls = pd.ExcelFile(file)
    excel_text = []
    for sheet_name in xls.sheet_names:
        df = xls.parse(sheet_name)
        excel_text.append(df.to_csv(index=False))
    return "\n".join(excel_text)


def extract_text_from_csv(file) -> str:
    df = pd.read_csv(file)
    return df.to_csv(index=False)


def chunk_text_by_tokens(text: str, chunk_tokens=512, overlap_tokens=50) -> list[str]:
    """
    Uses 'tiktoken' to split 'text' into ~chunk_tokens tokens, 
    with overlap_tokens overlap. Helps preserve sentence/paragraph coherence.
    """
    enc = tiktoken.get_encoding("cl100k_base")  # or whichever encoding is correct for your model
    input_ids = enc.encode(text)

    chunks = []
    start = 0
    while start < len(input_ids):
        end = start + chunk_tokens
        chunk_slice = input_ids[start:end]
        # Decode back to string
        chunk_str = enc.decode(chunk_slice)
        chunks.append(chunk_str)
        # Advance by chunk_tokens - overlap_tokens
        start += (chunk_tokens - overlap_tokens)

    return chunks


def build_faiss_index(embeddings):
    """
    Build a FAISS index from a list of (unnormalized) embedding vectors.
    We'll use IndexFlatL2 here. If you prefer IP, you can switch to IndexFlatIP.
    """
    if not embeddings:
        return None, None
    dim = embeddings[0].shape[0]

    # For unnormalized embeddings, we often use L2 distance (IndexFlatL2).
    index = faiss.IndexFlatL2(dim)
    embs_array = np.stack(embeddings)
    index.add(embs_array)
    return index, dim


def chat_completion_with_context(user_query, relevant_chunks):
    """
    Build a system message from the top relevant chunks,
    then call client.chat.completions.create(...) with model=...
    """
    context_text = "\n\n".join(relevant_chunks)
    
    # Refined system prompt
    system_prompt = (
        "You are a helpful assistant with knowledge of four documents:\n"
        "1. **Policy Document (Subject Matter Expert)**: contains the domain knowledge of scorecard metrics.\n"
        "2. **Methodology Document**: explains how to calculate scorecard metrics.\n"
        "3. **Formula File**: details on formulae to compute metrics based on the methodology.\n"
        "4. **CSV File**: provides the actual metrics data that are calculated using the formula file.\n\n"
        "Use the following context extracted from these documents to answer the user's question.\n\n"
        f"Context:\n{context_text}\n\n"
        "When answering:\n"
        "- Base your response solely on the information found in these documents.\n"
        "- If you are not certain or the information is not in the documents, say you don't know.\n"
        "- Do not invent or assume facts not provided.\n"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query},
    ]

    response = client.chat.completions.create(
        model=CHAT_MODEL_NAME,
        messages=messages,
        temperature=0.7,
    )
    return response.choices[0].message.content


# ------------------------------------------------------------------------------
# 3. Streamlit RAG Demo
# ------------------------------------------------------------------------------
def main():
    st.title("RAG Demo with Token-Based Chunking & Unnormalized Embeddings (L2 Distance)")

    # We'll store chunks, embeddings, and the FAISS index in session_state
    if "chunks" not in st.session_state:
        st.session_state.chunks = []
    if "chunk_embeddings" not in st.session_state:
        st.session_state.chunk_embeddings = []
    if "faiss_index" not in st.session_state:
        st.session_state.faiss_index = None
    if "doc_sources" not in st.session_state:
        st.session_state.doc_sources = []

    # 1) Document Upload & Token-Based Chunking
    st.header("Upload Documents and Build Index")

    policy_file = st.file_uploader("Policy PDF", type=["pdf"])
    methodology_file = st.file_uploader("Methodology PDF", type=["pdf"])
    formula_file = st.file_uploader("Formula Excel", type=["xlsx", "xls"])
    csv_file = st.file_uploader("CSV Data", type=["csv"])

    if st.button("Process Documents"):
        # Clear old data
        st.session_state.chunks = []
        st.session_state.chunk_embeddings = []
        st.session_state.faiss_index = None
        st.session_state.doc_sources = []

        # Example chunk settings
        CHUNK_TOKENS = 512
        OVERLAP_TOKENS = 50

        # Policy
        if policy_file is not None:
            policy_text = extract_text_from_pdf(policy_file)
            policy_chunks = chunk_text_by_tokens(policy_text, CHUNK_TOKENS, OVERLAP_TOKENS)
            for c in policy_chunks:
                st.session_state.chunks.append(c)
                st.session_state.doc_sources.append((c, "Policy PDF"))

        # Methodology
        if methodology_file is not None:
            methodology_text = extract_text_from_pdf(methodology_file)
            methodology_chunks = chunk_text_by_tokens(methodology_text, CHUNK_TOKENS, OVERLAP_TOKENS)
            for c in methodology_chunks:
                st.session_state.chunks.append(c)
                st.session_state.doc_sources.append((c, "Methodology PDF"))

        # Formula
        if formula_file is not None:
            formula_text = extract_text_from_excel(formula_file)
            formula_chunks = chunk_text_by_tokens(formula_text, CHUNK_TOKENS, OVERLAP_TOKENS)
            for c in formula_chunks:
                st.session_state.chunks.append(c)
                st.session_state.doc_sources.append((c, "Formula Excel"))

        # CSV
        if csv_file is not None:
            csv_text = extract_text_from_csv(csv_file)
            csv_chunks = chunk_text_by_tokens(csv_text, CHUNK_TOKENS, OVERLAP_TOKENS)
            for c in csv_chunks:
                st.session_state.chunks.append(c)
                st.session_state.doc_sources.append((c, "CSV"))

        st.write(f"Total chunks created: {len(st.session_state.chunks)}")

        # 2) Embed (without normalization) and Build FAISS Index
        if st.session_state.chunks:
            st.write("Generating embeddings in batches & building FAISS index...")

            BATCH_SIZE = 50
            embeddings = []
            all_chunks = st.session_state.chunks

            # Loop over chunks in groups of BATCH_SIZE
            for i in range(0, len(all_chunks), BATCH_SIZE):
                batch_texts = all_chunks[i : i + BATCH_SIZE]

                response = client.embeddings.create(
                    model=EMBEDDING_MODEL_NAME,
                    input=batch_texts
                )

                for obj in response.data:
                    embedding_vec = np.array(obj.embedding, dtype=np.float32)
                    # We do NOT normalize here; keep the raw embeddings
                    embeddings.append(embedding_vec)

            st.session_state.chunk_embeddings = embeddings

            # Build the FAISS index using L2 distance
            index, dim = build_faiss_index(embeddings)
            st.session_state.faiss_index = index

            st.success("FAISS index built successfully!")

    st.divider()

    # 3) Query Interface
    st.header("Ask Questions")
    user_query = st.text_input("Enter your query:")
    if st.button("Search & Answer"):
        if not st.session_state.faiss_index or not st.session_state.chunk_embeddings:
            st.warning("No documents have been processed yet. Please upload & process documents first.")
            return

        # Embed the user query
        response = client.embeddings.create(
            model=EMBEDDING_MODEL_NAME,
            input=[user_query]
        )
        query_emb_vec = np.array(response.data[0].embedding, dtype=np.float32).reshape(1, -1)

        # Similarity search in FAISS with L2
        top_k = 3
        D, I = st.session_state.faiss_index.search(query_emb_vec, top_k)

        # Retrieve the top chunks
        top_chunks = []
        for idx in I[0]:
            top_chunks.append(st.session_state.chunks[idx])

        # Chat Completion with relevant chunks
        assistant_reply = chat_completion_with_context(user_query, top_chunks)
        st.write("**Assistant**:", assistant_reply)

        # Optional: Show sources
        st.subheader("Top Relevant Chunks:")
        for rank, idx in enumerate(I[0]):
            st.markdown(f"**Rank {rank+1}, Distance {D[0][rank]:.4f}** (lower = closer in L2)")
            doc_label = st.session_state.doc_sources[idx][1]
            chunk_text_str = st.session_state.doc_sources[idx][0]
            st.text_area(
                label=f"Chunk from {doc_label}",
                value=chunk_text_str,
                height=100
            )


if __name__ == "__main__":
    main()
