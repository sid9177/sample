import os
import streamlit as st
from openai import AzureOpenAI
import PyPDF2
import pandas as pd
import numpy as np
import faiss
import io

# ------------------------------
# 1. Azure & Global Settings
# ------------------------------
EMBEDDING_MODEL_NAME = "my-embedding-model"  # e.g., "text-embedding-ada-002" or your Azure embedding deployment name
CHAT_MODEL_NAME = "Cata-GPT4-o"              # Your Azure ChatGPT model deployment name
API_VERSION = "2024-05-01-preview"
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT", "https://YOUR_AZURE_OPENAI_ENDPOINT_HERE")
AZURE_OPENAI_KEY = os.environ.get("AZURE_OPENAI_API_KEY", "YOUR_AZURE_OPENAI_API_KEY_HERE")

# Initialize a single AzureOpenAI client
client = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=API_VERSION,
)

# ------------------------------
# 2. Helper Functions
# ------------------------------
def extract_text_from_pdf(file) -> str:
    pdf_reader = PyPDF2.PdfReader(file)
    all_text = []
    for page in pdf_reader.pages:
        text = page.extract_text()
        if text:
            all_text.append(text)
    return "\n".join(all_text)

def extract_text_from_excel(file) -> str:
    xls = pd.ExcelFile(file)
    excel_text = []
    for sheet_name in xls.sheet_names:
        df = xls.parse(sheet_name)
        excel_text.append(df.to_csv(index=False))
    return "\n".join(excel_text)

def extract_text_from_csv(file) -> str:
    df = pd.read_csv(file)
    return df.to_csv(index=False)

def chunk_text(text, chunk_size=1000, overlap=200):
    """
    Splits 'text' into chunks of size 'chunk_size' with 'overlap' characters.
    For a more robust approach, consider token-based chunking with tiktoken.
    """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += (chunk_size - overlap)
    return chunks

def build_faiss_index(embeddings):
    """
    Build a FAISS index from a list of embedding vectors.
    Returns (index, dim).
    """
    if not embeddings:
        return None, None
    dim = embeddings[0].shape[0]
    index = faiss.IndexFlatIP(dim)  # Using inner-product for approximate cosine if vectors are normalized
    embs_array = np.stack(embeddings)
    index.add(embs_array)
    return index, dim


def chat_completion_with_context(user_query, relevant_chunks):
    """
    Build a system message from the top relevant chunks,
    then call client.chat.completions.create(...) with model=...
    """
    context_text = "\n\n".join(relevant_chunks)
    system_prompt = (
        "You are a helpful assistant. Use the following context to answer the user's question.\n\n"
        f"Context:\n{context_text}\n\n"
        "Only answer using the context above. If you're not sure, say you don't know.\n"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query},
    ]

    response = client.chat.completions.create(
        model=CHAT_MODEL_NAME,
        messages=messages,
        temperature=0.7,
    )
    return response.choices[0].message.content

# ------------------------------
# 3. Streamlit RAG Demo
# ------------------------------
def main():
    st.title("RAG Demo with Azure OpenAI + FAISS (Batch Embedding, client.chat.completions.create)")

    # We'll store chunks, embeddings, and the FAISS index in session_state
    if "chunks" not in st.session_state:
        st.session_state.chunks = []
    if "chunk_embeddings" not in st.session_state:
        st.session_state.chunk_embeddings = []
    if "faiss_index" not in st.session_state:
        st.session_state.faiss_index = None
    if "doc_sources" not in st.session_state:
        st.session_state.doc_sources = []

    # 1) Document Upload & Chunking
    st.header("Upload Documents and Build Index")

    policy_file = st.file_uploader("Policy PDF", type=["pdf"])
    methodology_file = st.file_uploader("Methodology PDF", type=["pdf"])
    formula_file = st.file_uploader("Formula Excel", type=["xlsx", "xls"])
    csv_file = st.file_uploader("CSV Data", type=["csv"])

    if st.button("Process Documents"):
        # Clear old data
        st.session_state.chunks = []
        st.session_state.chunk_embeddings = []
        st.session_state.faiss_index = None
        st.session_state.doc_sources = []

        # Policy
        if policy_file is not None:
            policy_text = extract_text_from_pdf(policy_file)
            for chunk in chunk_text(policy_text):
                st.session_state.chunks.append(chunk)
                st.session_state.doc_sources.append((chunk, "Policy PDF"))

        # Methodology
        if methodology_file is not None:
            methodology_text = extract_text_from_pdf(methodology_file)
            for chunk in chunk_text(methodology_text):
                st.session_state.chunks.append(chunk)
                st.session_state.doc_sources.append((chunk, "Methodology PDF"))

        # Formula
        if formula_file is not None:
            formula_text = extract_text_from_excel(formula_file)
            for chunk in chunk_text(formula_text):
                st.session_state.chunks.append(chunk)
                st.session_state.doc_sources.append((chunk, "Formula Excel"))

        # CSV
        if csv_file is not None:
            csv_text = extract_text_from_csv(csv_file)
            for chunk in chunk_text(csv_text):
                st.session_state.chunks.append(chunk)
                st.session_state.doc_sources.append((chunk, "CSV"))

        st.write(f"Total chunks created: {len(st.session_state.chunks)}")

        # 2) Embed and Build FAISS (Batch Embedding)
        if st.session_state.chunks:
            st.write("Generating embeddings in batches & building FAISS index...")

            BATCH_SIZE = 50
            embeddings = []
            all_chunks = st.session_state.chunks

            # Loop over chunks in groups of BATCH_SIZE
            for i in range(0, len(all_chunks), BATCH_SIZE):
                batch_texts = all_chunks[i : i + BATCH_SIZE]

                # Single request for multiple texts
                response = client.embeddings.create(
                    model=EMBEDDING_MODEL_NAME,
                    input=batch_texts
                )

                # Each item in response.data corresponds to one embedding
                for obj in response.data:
                    # Use dot notation for the embedding attribute
                    embeddings.append(np.array(obj.embedding, dtype=np.float32))

            # Store embeddings
            st.session_state.chunk_embeddings = embeddings

            # Build FAISS index
            index, dim = build_faiss_index(embeddings)
            st.session_state.faiss_index = index
            st.success("FAISS index built successfully!")

    st.divider()

    # 3) Query Interface
    st.header("Ask Questions")
    user_query = st.text_input("Enter your query:")
    if st.button("Search & Answer"):
        if not st.session_state.faiss_index or not st.session_state.chunk_embeddings:
            st.warning("No documents have been processed yet. Please upload & process documents first.")
            return

        # Embed the user query (just 1 item)
        response = client.embeddings.create(
            model=EMBEDDING_MODEL_NAME,
            input=[user_query]
        )
        query_emb = np.array(response.data[0].embedding, dtype=np.float32).reshape(1, -1)

        # Similarity search in FAISS
        top_k = 3
        D, I = st.session_state.faiss_index.search(query_emb, top_k)

        # Retrieve the top chunks
        top_chunks = []
        for idx in I[0]:
            top_chunks.append(st.session_state.chunks[idx])

        # Chat Completion with relevant chunks
        assistant_reply = chat_completion_with_context(user_query, top_chunks)
        st.write("**Assistant**:", assistant_reply)

        # Optional: Show sources
        st.subheader("Top Relevant Chunks:")
        for rank, idx in enumerate(I[0]):
            st.markdown(f"**Rank {rank+1}, Score {D[0][rank]:.4f}**")
            doc_label = st.session_state.doc_sources[idx][1]
            chunk_text_str = st.session_state.doc_sources[idx][0]
            st.text_area(
                label=f"Chunk from {doc_label}",
                value=chunk_text_str,
                height=80
            )

if __name__ == "__main__":
    main()
