import os
import streamlit as st
from openai import AzureOpenAI
import PyPDF2
import pandas as pd
import numpy as np
import faiss
import io

# Import the LangChain Text Splitter
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ------------------------------------------------------------------------------
# 1. Azure & Global Settings
# ------------------------------------------------------------------------------
EMBEDDING_MODEL_NAME = "text-embedding-ada-002"  # or your Azure embedding deployment name
CHAT_MODEL_NAME = "Cata-GPT4-o"                  # Your Azure ChatGPT model
API_VERSION = "2024-05-01-preview"
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT", "https://YOUR_AZURE_OPENAI_ENDPOINT_HERE")
AZURE_OPENAI_KEY = os.environ.get("AZURE_OPENAI_API_KEY", "YOUR_AZURE_OPENAI_API_KEY_HERE")

# Initialize a single AzureOpenAI client
client = AzureOpenAI(
    api_key=AZURE_OPENAI_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_version=API_VERSION,
)


# ------------------------------------------------------------------------------
# 2. Helper Functions
# ------------------------------------------------------------------------------
def extract_text_from_pdf(file) -> str:
    """Extract text from a PDF using PyPDF2."""
    pdf_reader = PyPDF2.PdfReader(file)
    all_text = []
    for page in pdf_reader.pages:
        text = page.extract_text()
        if text:
            all_text.append(text)
    return "\n".join(all_text)


def extract_text_from_excel(file) -> str:
    """Extract text from an Excel file (all sheets)."""
    xls = pd.ExcelFile(file)
    excel_text = []
    for sheet_name in xls.sheet_names:
        df = xls.parse(sheet_name)
        excel_text.append(df.to_csv(index=False))
    return "\n".join(excel_text)


def extract_text_from_csv(file) -> str:
    """Extract CSV content as text."""
    df = pd.read_csv(file)
    return df.to_csv(index=False)


def chunk_text_langchain(text: str, chunk_size=1000, chunk_overlap=200) -> list[str]:
    """
    Use LangChain's RecursiveCharacterTextSplitter to chunk the text.

    chunk_size and chunk_overlap are approximate character-based settings.
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""],  # tries to split more naturally
    )
    # split_text returns a list of strings (in LangChain >=0.0.91)
    chunks = text_splitter.split_text(text)
    return chunks


def build_faiss_index(embeddings):
    """
    Build a FAISS index from a list of embedding vectors (unnormalized).
    We'll use IndexFlatL2 for simplicity.
    """
    if not embeddings:
        return None, None
    dim = embeddings[0].shape[0]
    index = faiss.IndexFlatL2(dim)
    embs_array = np.stack(embeddings)
    index.add(embs_array)
    return index, dim


def chat_completion_with_context(user_query, relevant_chunks):
    """
    Build a system message from the top relevant chunks,
    then call client.chat.completions.create(...) with model=...
    """
    context_text = "\n\n".join(relevant_chunks)

    # Refine your system prompt to reflect each document's role if desired:
    system_prompt = (
        "You are a helpful assistant with knowledge of four documents:\n"
        "1. **Policy Document** (Subject Matter Expert) - domain knowledge of scorecard metrics.\n"
        "2. **Methodology Document** - explains how to calculate scorecard metrics.\n"
        "3. **Formula File** - details on formulae to compute metrics.\n"
        "4. **CSV File** - metrics data, calculated by the formula.\n\n"
        "Use the following context from these documents to answer.\n\n"
        f"Context:\n{context_text}\n\n"
        "If you don't find the answer in these documents, say you don't know.\n"
        "Do not invent facts."
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query},
    ]

    response = client.chat.completions.create(
        model=CHAT_MODEL_NAME,
        messages=messages,
        temperature=0.7,
    )
    return response.choices[0].message.content


# ------------------------------------------------------------------------------
# 3. Streamlit RAG Demo
# ------------------------------------------------------------------------------
def main():
    st.title("RAG Demo with LangChain Chunking & FAISS")

    # We'll store chunks, embeddings, and the FAISS index in session_state
    if "chunks" not in st.session_state:
        st.session_state.chunks = []
    if "chunk_embeddings" not in st.session_state:
        st.session_state.chunk_embeddings = []
    if "faiss_index" not in st.session_state:
        st.session_state.faiss_index = None
    if "doc_sources" not in st.session_state:
        st.session_state.doc_sources = []

    # 1) Document Upload & Chunking
    st.header("Upload Documents and Build Index")

    policy_file = st.file_uploader("Policy PDF", type=["pdf"])
    methodology_file = st.file_uploader("Methodology PDF", type=["pdf"])
    formula_file = st.file_uploader("Formula Excel", type=["xlsx", "xls"])
    csv_file = st.file_uploader("CSV Data", type=["csv"])

    if st.button("Process Documents"):
        # Clear old data
        st.session_state.chunks = []
        st.session_state.chunk_embeddings = []
        st.session_state.faiss_index = None
        st.session_state.doc_sources = []

        # Example chunk settings (approx chars)
        CHUNK_SIZE = 1000
        OVERLAP_SIZE = 200

        # Policy
        if policy_file is not None:
            policy_text = extract_text_from_pdf(policy_file)
            policy_chunks = chunk_text_langchain(policy_text, CHUNK_SIZE, OVERLAP_SIZE)
            for c in policy_chunks:
                st.session_state.chunks.append(c)
                st.session_state.doc_sources.append((c, "Policy PDF"))

        # Methodology
        if methodology_file is not None:
            methodology_text = extract_text_from_pdf(methodology_file)
            methodology_chunks = chunk_text_langchain(methodology_text, CHUNK_SIZE, OVERLAP_SIZE)
            for c in methodology_chunks:
                st.session_state.chunks.append(c)
                st.session_state.doc_sources.append((c, "Methodology PDF"))

        # Formula
        if formula_file is not None:
            formula_text = extract_text_from_excel(formula_file)
            formula_chunks = chunk_text_langchain(formula_text, CHUNK_SIZE, OVERLAP_SIZE)
            for c in formula_chunks:
                st.session_state.chunks.append(c)
                st.session_state.doc_sources.append((c, "Formula Excel"))

        # CSV
        if csv_file is not None:
            csv_text = extract_text_from_csv(csv_file)
            csv_chunks = chunk_text_langchain(csv_text, CHUNK_SIZE, OVERLAP_SIZE)
            for c in csv_chunks:
                st.session_state.chunks.append(c)
                st.session_state.doc_sources.append((c, "CSV"))

        st.write(f"Total chunks created: {len(st.session_state.chunks)}")

        # 2) Embed and Build FAISS Index
        if st.session_state.chunks:
            st.write("Generating embeddings in batches & building FAISS index...")

            BATCH_SIZE = 50
            embeddings = []
            all_chunks = st.session_state.chunks

            # Loop over chunks in groups of BATCH_SIZE
            for i in range(0, len(all_chunks), BATCH_SIZE):
                batch_texts = all_chunks[i : i + BATCH_SIZE]

                response = client.embeddings.create(
                    model=EMBEDDING_MODEL_NAME,
                    input=batch_texts
                )
                # Each item in response.data corresponds to one embedding
                for obj in response.data:
                    embeddings.append(np.array(obj.embedding, dtype=np.float32))

            st.session_state.chunk_embeddings = embeddings
            # Build the FAISS index
            index, dim = build_faiss_index(embeddings)
            st.session_state.faiss_index = index
            st.success("FAISS index built successfully!")

    st.divider()

    # 3) Query Interface
    st.header("Ask Questions")
    user_query = st.text_input("Enter your query:")
    if st.button("Search & Answer"):
        if not st.session_state.faiss_index or not st.session_state.chunk_embeddings:
            st.warning("No documents have been processed yet. Please upload & process documents first.")
            return

        # Embed the user query
        response = client.embeddings.create(
            model=EMBEDDING_MODEL_NAME,
            input=[user_query]
        )
        query_emb_vec = np.array(response.data[0].embedding, dtype=np.float32).reshape(1, -1)

        # Similarity search in FAISS
        top_k = 3
        D, I = st.session_state.faiss_index.search(query_emb_vec, top_k)

        # Retrieve the top chunks
        top_chunks = []
        for idx in I[0]:
            top_chunks.append(st.session_state.chunks[idx])

        # Chat Completion with relevant chunks
        assistant_reply = chat_completion_with_context(user_query, top_chunks)
        st.write("**Assistant**:", assistant_reply)

        # Optional: Show sources
        st.subheader("Top Relevant Chunks:")
        for rank, idx in enumerate(I[0]):
            st.markdown(f"**Rank {rank+1}, Distance {D[0][rank]:.4f}**")
            doc_label = st.session_state.doc_sources[idx][1]
            chunk_text_str = st.session_state.doc_sources[idx][0]
            st.text_area(
                label=f"Chunk from {doc_label}",
                value=chunk_text_str,
                height=100
            )


if __name__ == "__main__":
    main()
